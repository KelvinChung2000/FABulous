{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd9356d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NextPNR Parameter Sweep with MLIR Processing Pipeline ===\n",
      "Comprehensive FPGA parameter optimization using MLIR-generated benchmarks\n",
      "Auto-reload enabled for module imports\n",
      "âœ“ Core libraries and logging configured\n"
     ]
    }
   ],
   "source": [
    "# NextPNR Parameter Sweep with MLIR Processing Pipeline\n",
    "# Comprehensive tool for FPGA parameter optimization using MLIR-generated benchmarks\n",
    "\n",
    "# Enable auto reload for imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Standard library imports\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import concurrent.futures\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Configure loguru logger - output only to stdout (only if not already configured)\n",
    "# logger.remove()  # Remove default handler\n",
    "# logger.add(sys.stdout, format=\"{time} | {level} | {message}\", level=\"INFO\")\n",
    "\n",
    "print(\"=== NextPNR Parameter Sweep with MLIR Processing Pipeline ===\")\n",
    "print(\"Comprehensive FPGA parameter optimization using MLIR-generated benchmarks\")\n",
    "print(\"Auto-reload enabled for module imports\")\n",
    "\n",
    "print(\"âœ“ Core libraries and logging configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abde904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration setup complete:\n",
      "  Parameter combinations: 10 Ã— 10 = 100 total\n",
      "  Test grid size: 100 combinations\n",
      "  Compilation results directory: /home/kelvin/FABulous_fork/myProject/PnR/compilation_result\n",
      "  Stage directories:\n",
      "    01_optimized_mlir/ - MLIR optimization results\n",
      "    02_extracted_mlir/ - Loop-to-function extraction results\n",
      "    03_intermediate_mlir/ - Individual function MLIR files\n",
      "    04_futil/ - Generated FUTIL files\n",
      "    05_verilog/ - Final Verilog output\n",
      "  Parameter sweep results directory: /home/kelvin/FABulous_fork/myProject/PnR/parameter_sweep_results\n",
      "  Benchmark root directory: /home/kelvin/FABulous_fork/myProject/PnR/mlir\n",
      "  Starting with unoptimized MLIR files from BENCHMARK folder\n",
      "âœ“ Configuration and paths set up successfully\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Parameter Setup\n",
    "# Define parameter ranges, paths, and constants for the parameter sweep\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define parameter ranges for the sweep\n",
    "CONNECTIVITY_FACTORS = np.arange(0.0, 2.0, 0.2)  # 0.0 to 2.0 with step 0.2\n",
    "CONGESTION_FACTORS = np.arange(0.0, 2.0, 0.2)    # 0.0 to 2.0 with step 0.2\n",
    "\n",
    "# Create parameter grid - use reduced set for testing or full grid for production\n",
    "grid = list(itertools.product(CONNECTIVITY_FACTORS, CONGESTION_FACTORS))  # Full grid\n",
    "# grid = [(0, 0), (0, 1), (1, 0), (1, 1)]  # Reduced grid for testing\n",
    "\n",
    "# software dir configuration\n",
    "CALYX_PATH: str = \"/home/kelvin/calyx\"\n",
    "MLIR_OPT_PATH: str = \"/home/kelvin/circt/build/vscode/bin/mlir-opt\"\n",
    "HLSTOOL_PATH: str = \"/home/kelvin/circt/build/vscode/bin/hlstool\"\n",
    "\n",
    "# Project and directory configuration\n",
    "MY_FAB_ROOT: Path = Path(\"/home/kelvin/FABulous_fork\")\n",
    "FAB_PROJ_DIR: Path = Path(\"/home/kelvin/FABulous_fork/myProject\")\n",
    "\n",
    "# Compilation results directory structure - Stage-based organization\n",
    "COMPILATION_RESULT_DIR: Path = Path(\"/home/kelvin/FABulous_fork/myProject/PnR/compilation_result\")\n",
    "\n",
    "# Stage-specific directories for cleaner organization and simpler naming\n",
    "MLIR_OPTIMIZED_DIR: Path = COMPILATION_RESULT_DIR / \"01_optimized_mlir\"\n",
    "MLIR_EXTRACTED_DIR: Path = COMPILATION_RESULT_DIR / \"02_extracted_mlir\"\n",
    "FUTIL_OUTPUT_DIR: Path = COMPILATION_RESULT_DIR / \"03_futil\"\n",
    "VERILOG_OUTPUT_DIR: Path = COMPILATION_RESULT_DIR / \"04_verilog\"\n",
    "SYNTHESIS_OUTPUT_DIR: Path = COMPILATION_RESULT_DIR / \"05_synthesis\"\n",
    "PNR_OUTPUT_DIR: Path = COMPILATION_RESULT_DIR / \"06_PNR\"\n",
    "OUTPUT_DIR: Path = Path(\"/home/kelvin/FABulous_fork/myProject/PnR/parameter_sweep_results\")\n",
    "\n",
    "# Benchmark directory configuration - MLIR files only\n",
    "BENCHMARK_ROOT_DIR: Path = Path(\"/home/kelvin/FABulous_fork/myProject/PnR/mlir\")\n",
    "\n",
    "# Set up environment variables\n",
    "os.environ[\"FAB_PROJ_DIR\"] = str(FAB_PROJ_DIR)\n",
    "os.environ[\"PATH\"] = f\"/home/kelvin/nextpnr/build/bba:{os.environ['PATH']}\"\n",
    "os.environ[\"PATH\"] = f\"/home/kelvin/nextpnr/build:{os.environ['PATH']}\"\n",
    "os.environ[\"PATH\"] = f\"/home/kelvin/yosys:{os.environ['PATH']}\"\n",
    "os.environ[\"PATH\"] = f\"/home/kelvin/circt/build/vscode/bin:{os.environ['PATH']}\"\n",
    "os.environ[\"CALYX_PRIMITIVES_DIR\"] = str(Path(CALYX_PATH))\n",
    "\n",
    "# Create necessary directories for all processing stages\n",
    "COMPILATION_RESULT_DIR.mkdir(exist_ok=True)\n",
    "MLIR_OPTIMIZED_DIR.mkdir(exist_ok=True)\n",
    "MLIR_EXTRACTED_DIR.mkdir(exist_ok=True)\n",
    "FUTIL_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "VERILOG_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# FABulous file paths\n",
    "CHIPDB_PATH = FAB_PROJ_DIR / \".FABulous/hycube.bit\"\n",
    "JSON_INPUT = FAB_PROJ_DIR / \"user_design/synth_test.json\"\n",
    "CONSTRAIN_PAIR = FAB_PROJ_DIR / \".FABulous/hycube_constrain_pair.inc\"\n",
    "FDC_PATH = FAB_PROJ_DIR / \"user_design/test.fdc\"\n",
    "\n",
    "# Fallback source file if no generated Verilog files are available\n",
    "FALLBACK_SOURCE_HDL = MY_FAB_ROOT / \"benchmarks/userbench/loop_array_inner/loop_array_inner.sv\"\n",
    "\n",
    "# Fixed parameters for nextpnr runs\n",
    "BETA_VALUE = 0.9\n",
    "PLACE_TRIALS = 1\n",
    "ROUTER_TIMEOUT = 20000\n",
    "\n",
    "# Initialize available MLIR files and Verilog files structures\n",
    "availableMlirFiles = []\n",
    "availableVerilogFiles = []\n",
    "benchmarkStructure = {}\n",
    "\n",
    "print(\"Configuration setup complete:\")\n",
    "print(f\"  Parameter combinations: {len(CONNECTIVITY_FACTORS)} Ã— {len(CONGESTION_FACTORS)} = {len(CONNECTIVITY_FACTORS) * len(CONGESTION_FACTORS)} total\")\n",
    "print(f\"  Test grid size: {len(grid)} combinations\")\n",
    "print(f\"  Compilation results directory: {COMPILATION_RESULT_DIR}\")\n",
    "print(\"  Stage directories:\")\n",
    "print(\"    01_optimized_mlir/ - MLIR optimization results\")\n",
    "print(\"    02_extracted_mlir/ - Loop-to-function extraction results\")\n",
    "print(\"    03_intermediate_mlir/ - Individual function MLIR files\")\n",
    "print(\"    04_futil/ - Generated FUTIL files\")\n",
    "print(\"    05_verilog/ - Final Verilog output\")\n",
    "print(f\"  Parameter sweep results directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Benchmark root directory: {BENCHMARK_ROOT_DIR}\")\n",
    "print(\"  Starting with unoptimized MLIR files from BENCHMARK folder\")\n",
    "\n",
    "print(\"âœ“ Configuration and paths set up successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377a4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data structures for complete pipeline\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "from enum import Enum\n",
    "import time\n",
    "import os\n",
    "\n",
    "class FailureType(Enum):\n",
    "    NONE = \"none\"\n",
    "    MLIR_OPTIMIZATION = \"mlir_optimization\"\n",
    "    LOOP_EXTRACTION = \"loop_extraction\" \n",
    "    FUTIL_GENERATION = \"futil_generation\"\n",
    "    VERILOG_GENERATION = \"verilog_generation\"\n",
    "    SYNTHESIS = \"synthesis\"\n",
    "    PLACEMENT = \"placement\"\n",
    "    ROUTING = \"routing\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "@dataclass\n",
    "class VerilogPipelineResult:\n",
    "    \"\"\"Unified MLIR to Verilog generation pipeline result\"\"\"\n",
    "    source_file: Path\n",
    "    success: bool\n",
    "    runtime: float\n",
    "    \n",
    "    # Output files\n",
    "    optimized_mlir_file: Optional[Path] = None\n",
    "    function_extracted: Optional[list[Path]] = None\n",
    "    futil_files: Optional[list[Path]] = None\n",
    "    verilog_files: Optional[list[Path]] = None\n",
    "    \n",
    "    # Stage metrics\n",
    "    mlir_optimization_time: float = 0.0\n",
    "    loop_extraction_time: float = 0.0\n",
    "    futil_generation_time: float = 0.0\n",
    "\n",
    "    \n",
    "    # Error tracking\n",
    "    failure_stage: Optional[FailureType] = None\n",
    "    error_message: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.verilog_files is None:\n",
    "            self.verilog_files = []\n",
    "\n",
    "@dataclass\n",
    "class SynthesisResult:\n",
    "    \"\"\"FABulous synthesis preprocessing result\"\"\"\n",
    "    success: bool\n",
    "    runtime: float\n",
    "    verilog_file: Optional[Path] = None\n",
    "    synthesis_out_file: Optional[Path] = None\n",
    "    error_message: Optional[str] = None\n",
    "    full_stdout: Optional[str] = None\n",
    "    full_stderr: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class ParameterResult:\n",
    "    \"\"\"Single parameter configuration result\"\"\"\n",
    "    config_id: int\n",
    "    success: bool\n",
    "    runtime: float\n",
    "    \n",
    "    # Configuration parameters\n",
    "    parameters: Dict[str, Any] = None\n",
    "    \n",
    "    # NextPNR analysis results\n",
    "    placement_info: Optional[Dict[str, Any]] = None\n",
    "    routing_info: Optional[Dict[str, Any]] = None\n",
    "\n",
    "    full_stdout: Optional[str] = None\n",
    "    full_stderr: Optional[str] = None\n",
    "    \n",
    "    # Error tracking\n",
    "    failure_type: Optional[FailureType] = None\n",
    "    error_message: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.parameters is None:\n",
    "            self.parameters = {}\n",
    "\n",
    "@dataclass\n",
    "class CompletePipelineResult:\n",
    "    \"\"\"Complete end-to-end pipeline result\"\"\"\n",
    "    source_file: Path\n",
    "    overall_success: bool\n",
    "    total_runtime: float\n",
    "    \n",
    "    # Stage results\n",
    "    verilog_pipeline: VerilogPipelineResult\n",
    "    synthesis_results: List[SynthesisResult]  # Updated to handle multiple synthesis results\n",
    "    parameter_results: List[ParameterResult]\n",
    "    \n",
    "    # Overall failure tracking\n",
    "    primary_failure_type: FailureType = FailureType.NONE\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for DataFrame creation\"\"\"\n",
    "        base_dict = {\n",
    "            'source_file': os.path.basename(self.source_file),\n",
    "            'overall_success': self.overall_success,\n",
    "            'total_runtime': self.total_runtime,\n",
    "            'primary_failure': self.primary_failure_type.value,\n",
    "            \n",
    "            # Verilog pipeline\n",
    "            'verilog_success': self.verilog_pipeline.success,\n",
    "            'verilog_runtime': self.verilog_pipeline.runtime,\n",
    "            'mlir_opt_time': self.verilog_pipeline.mlir_optimization_time,\n",
    "            'loop_extract_time': self.verilog_pipeline.loop_extraction_time,\n",
    "            'futil_gen_time': self.verilog_pipeline.futil_generation_time,\n",
    "            'function_extracted': self.verilog_pipeline.function_extracted,\n",
    "            \n",
    "            # Synthesis - aggregated metrics from multiple synthesis results\n",
    "            'synthesis_count': len(self.synthesis_results),\n",
    "            'synthesis_success_count': sum(1 for s in self.synthesis_results if s.success),\n",
    "            'synthesis_overall_success': any(s.success for s in self.synthesis_results),\n",
    "            'synthesis_total_runtime': sum(s.runtime for s in self.synthesis_results),\n",
    "            'synthesis_avg_runtime': sum(s.runtime for s in self.synthesis_results) / len(self.synthesis_results) if self.synthesis_results else 0,\n",
    "            \n",
    "            # Parameter sweep summary\n",
    "            'total_configs': len(self.parameter_results),\n",
    "            'successful_configs': sum(1 for p in self.parameter_results if p.success),\n",
    "        }\n",
    "        \n",
    "        if self.parameter_results:\n",
    "            successful_params = [p for p in self.parameter_results if p.success]\n",
    "            if successful_params:\n",
    "                base_dict['avg_param_runtime'] = sum(p.runtime for p in successful_params) / len(successful_params)\n",
    "                base_dict['min_param_runtime'] = min(p.runtime for p in successful_params)\n",
    "                base_dict['max_param_runtime'] = max(p.runtime for p in successful_params)\n",
    "        \n",
    "        return base_dict\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        String representation for displaying pipeline result information.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Formatted summary of the pipeline result.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        lines.append(f\"Pipeline Result for: {self.source_file}\")\n",
    "        lines.append(f\"  Overall Success: {self.overall_success}\")\n",
    "        lines.append(f\"  Total Runtime: {self.total_runtime:.2f}s\")\n",
    "        lines.append(f\"  Primary Failure: {self.primary_failure_type.value}\")\n",
    "        lines.append(\"  Verilog Pipeline:\")\n",
    "        lines.append(f\"    Success: {self.verilog_pipeline.success}\")\n",
    "        lines.append(f\"    Runtime: {self.verilog_pipeline.runtime:.2f}s\")\n",
    "        lines.append(f\"    MLIR Opt Time: {self.verilog_pipeline.mlir_optimization_time:.2f}s\")\n",
    "        lines.append(f\"    Loop Extract Time: {self.verilog_pipeline.loop_extraction_time:.2f}s\")\n",
    "        lines.append(f\"    FUTIL Gen Time: {self.verilog_pipeline.futil_generation_time:.2f}s\")\n",
    "        if self.verilog_pipeline.function_extracted:\n",
    "            lines.append(f\"    function extracted: {self.verilog_pipeline.function_extracted}\")\n",
    "        if self.verilog_pipeline.failure_stage:\n",
    "            lines.append(f\"    Failure Stage: {self.verilog_pipeline.failure_stage.value}\")\n",
    "        if self.verilog_pipeline.error_message:\n",
    "            lines.append(f\"    Error: {self.verilog_pipeline.error_message}\")\n",
    "        \n",
    "        # Updated synthesis section to handle multiple results\n",
    "        lines.append(\"  Synthesis:\")\n",
    "        lines.append(f\"    Total Synthesis Jobs: {len(self.synthesis_results)}\")\n",
    "        successful_synthesis = sum(1 for s in self.synthesis_results if s.success)\n",
    "        lines.append(f\"    Successful: {successful_synthesis}/{len(self.synthesis_results)}\")\n",
    "        if self.synthesis_results:\n",
    "            total_synthesis_runtime = sum(s.runtime for s in self.synthesis_results)\n",
    "            lines.append(f\"    Total Runtime: {total_synthesis_runtime:.2f}s\")\n",
    "            lines.append(f\"    Average Runtime: {total_synthesis_runtime/len(self.synthesis_results):.2f}s\")\n",
    "            \n",
    "            # Show details for failed synthesis\n",
    "            failed_synthesis = [s for s in self.synthesis_results if not s.success]\n",
    "            if failed_synthesis:\n",
    "                lines.append(f\"    Failed synthesis errors:\")\n",
    "                for i, s in enumerate(failed_synthesis):\n",
    "                    if s.error_message:\n",
    "                        lines.append(f\"      {i+1}. {s.error_message}\")\n",
    "        \n",
    "        lines.append(\"  Parameter Sweep:\")\n",
    "        lines.append(f\"    Total Configs: {len(self.parameter_results)}\")\n",
    "        successful_params = sum(1 for p in self.parameter_results if p.success)\n",
    "        lines.append(f\"    Successful Configs: {successful_params}\")\n",
    "        if self.parameter_results:\n",
    "            failures = [p for p in self.parameter_results if not p.success]\n",
    "            if failures:\n",
    "                lines.append(f\"    Failed Configs: {len(failures)}\")\n",
    "                failure_types = {}\n",
    "                for p in failures:\n",
    "                    ft = p.failure_type.value if p.failure_type else \"unknown\"\n",
    "                    failure_types[ft] = failure_types.get(ft, 0) + 1\n",
    "                for ft, count in failure_types.items():\n",
    "                    lines.append(f\"      {ft}: {count}\")\n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10fa5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct pipeline functions with proper stage directories\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "def run_verilog_pipeline(mlir_file: Path) -> VerilogPipelineResult:\n",
    "    \"\"\"\n",
    "    Run the complete MLIR to Verilog generation pipeline with stage directories\n",
    "    MLIR optimization -> Loop extraction -> FUTIL generation -> Verilog generation\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    mlir_path = Path(mlir_file)\n",
    "    base_name = mlir_path.stem\n",
    "    \n",
    "    result = VerilogPipelineResult(\n",
    "        source_file=mlir_file,\n",
    "        success=False,\n",
    "        runtime=0.0,\n",
    "        verilog_files=[]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: MLIR Optimization\n",
    "        mlir_start = time.time()\n",
    "        optimized_mlir_path = MLIR_OPTIMIZED_DIR / f\"{base_name}.mlir\"\n",
    "        result.optimized_mlir_file, mlir_success, mlir_error = run_mlir_optimization(mlir_path, optimized_mlir_path)\n",
    "        result.mlir_optimization_time = time.time() - mlir_start\n",
    "        \n",
    "        if not mlir_success or result.optimized_mlir_file is None:\n",
    "            result.failure_stage = FailureType.MLIR_OPTIMIZATION\n",
    "            result.error_message = mlir_error or \"MLIR optimization failed\"\n",
    "            result.runtime = time.time() - start_time\n",
    "            return result\n",
    "        \n",
    "        # Stage 2: Loop Extraction - now returns list of files\n",
    "        loop_start = time.time()\n",
    "        result.function_extracted, loop_success, loop_error = extract_loops_from_mlir(Path(result.optimized_mlir_file), MLIR_EXTRACTED_DIR)\n",
    "        result.loop_extraction_time = time.time() - loop_start\n",
    "        \n",
    "        if not loop_success:\n",
    "            result.failure_stage = FailureType.LOOP_EXTRACTION\n",
    "            result.error_message = loop_error or \"Loop extraction failed\"\n",
    "            result.runtime = time.time() - start_time\n",
    "            return result\n",
    "        \n",
    "        # Stage 3 & 4: Process each extracted file through FUTIL -> Verilog\n",
    "        all_verilog_files = []\n",
    "        all_futil_files = []\n",
    "        for extracted_file in result.function_extracted:\n",
    "            # FUTIL Generation for this file\n",
    "            futil_start = time.time()\n",
    "            futil_file, futil_success, futil_error = generate_futil_from_mlir(extracted_file, FUTIL_OUTPUT_DIR)\n",
    "            all_futil_files.append(futil_file)\n",
    "            futil_time = time.time() - futil_start\n",
    "            result.futil_generation_time += futil_time\n",
    "            \n",
    "            if not futil_success or futil_file is None:\n",
    "                result.failure_stage = FailureType.FUTIL_GENERATION\n",
    "                result.error_message = futil_error or f\"FUTIL generation failed for {extracted_file}\"\n",
    "                result.runtime = time.time() - start_time\n",
    "                return result\n",
    "            \n",
    "            # Verilog Generation for this FUTIL file\n",
    "            verilog_file, verilog_success, verilog_error = generate_verilog_from_futil(futil_file, VERILOG_OUTPUT_DIR)\n",
    "            \n",
    "            if not verilog_success or not verilog_file:\n",
    "                result.failure_stage = FailureType.VERILOG_GENERATION\n",
    "                result.error_message = verilog_error or f\"Verilog generation failed for {futil_file}\"\n",
    "                result.runtime = time.time() - start_time\n",
    "                return result\n",
    "                \n",
    "            all_verilog_files.append(verilog_file)\n",
    "        \n",
    "        result.verilog_files = all_verilog_files\n",
    "        result.futil_files = all_futil_files\n",
    "        result.success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        result.failure_stage = FailureType.VERILOG_GENERATION\n",
    "        result.error_message = str(e)\n",
    "    \n",
    "    result.runtime = time.time() - start_time\n",
    "    return result\n",
    "\n",
    "def run_mlir_optimization(mlir_file: Path, output_path: Path) -> Tuple[Optional[Path], bool, Optional[str]]:\n",
    "    \"\"\"Run MLIR optimization passes - returns (output_file, success, error_message)\"\"\"\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        cmd = [\n",
    "            \"mlir-opt\",\n",
    "            \"--allow-unregistered-dialect\",\n",
    "            \"--int-range-optimizations\",\n",
    "            \"--sroa\",\n",
    "            \"--normalize-memrefs\", \n",
    "            \"--flatten-memref\",\n",
    "            \"--enable-loop-simplifycfg-term-folding\",\n",
    "            \"--affine-loop-fusion\",\n",
    "            \"--affine-simplify-structures\",\n",
    "            \"--affine-loop-invariant-code-motion\",\n",
    "            \"--affine-scalrep\",\n",
    "            \"--affine-pipeline-data-transfer\",\n",
    "            \"-mlir-print-op-generic\",\n",
    "            str(mlir_file),\n",
    "            '-o', str(output_path)\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        if result.returncode == 0 and output_path.exists():\n",
    "            return output_path, True, None\n",
    "        else:\n",
    "            error_msg = f\"MLIR optimization failed: {result.stderr}\"\n",
    "            return None, False, error_msg\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return None, False, \"MLIR optimization timed out\"\n",
    "    except Exception as e:\n",
    "        return None, False, f\"MLIR optimization error: {str(e)}\"\n",
    "\n",
    "def extract_loops_from_mlir(optimized_mlir_file: Path, extracted_dir: Path) -> Tuple[List[Path], bool, Optional[str]]:\n",
    "    \"\"\"Extract loops using inner_loop_to_func - returns (list_of_extracted_files, success, error_message)\"\"\"\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        extracted_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        from inner_loop_to_func import process_mlir_file\n",
    "        extracted_file_paths = process_mlir_file(str(optimized_mlir_file), str(extracted_dir))\n",
    "        if extracted_file_paths:\n",
    "            return extracted_file_paths, True, None\n",
    "        else:\n",
    "            return [], False, \"No loops extracted from MLIR file\"\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        return [], False, f\"MLIR file not found: {optimized_mlir_file}\"\n",
    "    except Exception as e:\n",
    "        return [], False, f\"Loop extraction error: {str(e)}\"\n",
    "\n",
    "def generate_futil_from_mlir(mlir_file: Path, futil_dir: Path) -> Tuple[Optional[Path], bool, Optional[str]]:\n",
    "    \"\"\"Generate FUTIL file from a single MLIR file - returns (futil_file, success, error_message)\"\"\"\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        futil_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        futil_path = futil_dir / f\"{mlir_file.stem}.futil\"\n",
    "        intermediate_mlir = futil_dir / f\"{mlir_file.stem}.mlir\"\n",
    "        \n",
    "        try:\n",
    "            # Step 1: hlstool command\n",
    "            hlstool_cmd = [\n",
    "                \"hlstool\",\n",
    "                \"--calyx-hw\",\n",
    "                \"--output-level=core\", \n",
    "                \"--ir\",\n",
    "                \"--allow-unregistered-dialects\",\n",
    "                str(mlir_file),\n",
    "                \"-o\", str(intermediate_mlir)\n",
    "            ]\n",
    "            \n",
    "            result = subprocess.run(hlstool_cmd, capture_output=True, text=True, timeout=30)\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                return None, False, f\"hlstool failed for {mlir_file.name}: {result.stderr}\"\n",
    "            \n",
    "            # Step 2: circt-translate command\n",
    "            translate_cmd = [\n",
    "                \"circt-translate\",\n",
    "                str(intermediate_mlir),\n",
    "                \"-o\", str(futil_path),\n",
    "                \"--export-calyx\"\n",
    "            ]\n",
    "            \n",
    "            result = subprocess.run(translate_cmd, capture_output=True, text=True, timeout=60)\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                return None, False, f\"circt-translate failed for {mlir_file.name}: {result.stderr}\"\n",
    "            \n",
    "            # Clean up intermediate file\n",
    "            intermediate_mlir.unlink(missing_ok=True)\n",
    "            \n",
    "            return futil_path, True, None\n",
    "            \n",
    "        except subprocess.TimeoutExpired:\n",
    "            return None, False, f\"Timeout processing {mlir_file.name}\"\n",
    "        except Exception as e:\n",
    "            return None, False, f\"Error processing {mlir_file.name}: {e}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return None, False, f\"FUTIL generation error: {str(e)}\"\n",
    "\n",
    "def generate_verilog_from_futil(futil_file: Path, verilog_dir: Path) -> Tuple[Optional[Path], bool, Optional[str]]:\n",
    "    \"\"\"Generate Verilog files from FUTIL - returns (verilog_file, success, error_message)\"\"\"\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        verilog_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        verilog_path = verilog_dir / f\"{futil_file.stem}.sv\"\n",
    "        \n",
    "        # Calyx compilation command\n",
    "        calyx_cmd = [\n",
    "            \"/home/kelvin/calyx/target/debug/calyx\",\n",
    "            \"-l\", \"/home/kelvin/calyx\",  # Add library path\n",
    "            \"-p\", \"fsm-opt\",\n",
    "            \"-x\", \"simplify-with-control:without-register\",\n",
    "            \"-x\", \"static-inline:offload-pause=false\", \n",
    "            \"-p\", \"lower\",\n",
    "            \"--nested\",\n",
    "            \"-d\", \"papercut\",\n",
    "            \"-d\", \"cell-share\",\n",
    "            \"-o\", str(verilog_path),\n",
    "            \"-b\", \"verilog\",\n",
    "            \"--synthesis\",\n",
    "            str(futil_file)\n",
    "        ]\n",
    "        \n",
    "        env = os.environ.copy()\n",
    "        result = subprocess.run(calyx_cmd, capture_output=True, text=True, timeout=300, cwd=str(CALYX_PATH), env=env)\n",
    "        \n",
    "        if result.returncode == 0 and verilog_path.exists():\n",
    "            return verilog_path, True, None\n",
    "        else:\n",
    "            error_msg = f\"Verilog generation failed: {result.stderr}\"\n",
    "            return None, False, error_msg\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return None, False, \"Verilog generation timed out\"\n",
    "    except Exception as e:\n",
    "        return None, False, f\"Verilog generation error: {str(e)}\"\n",
    "\n",
    "def run_synthesis(verilog_file: Path) -> SynthesisResult:\n",
    "    \"\"\"Run FABulous synthesis preprocessing only - returns SynthesisResult directly\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Run FABulous synthesis preprocessing (yosys + json generation)\n",
    "        synthesis_cmd = [\n",
    "            \"FABulous\", \"--debug\", str(FAB_PROJ_DIR), \"-p\",\n",
    "            \"load_fabric; gen_bitStream_spec; gen_cells_and_techmaps; \"\n",
    "            f\"gen_chipdb -routing_graph {FAB_PROJ_DIR}/.FABulous/routing_graph.dot -filter 5,1 5,2 5,3 5,4; \"\n",
    "            f\"synthesis_script {str(verilog_file)} -tcl {FAB_PROJ_DIR}/.FABulous/arch_synth.tcl;\"\n",
    "        ]\n",
    "        SYNTHESIS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        synthesis_out = SYNTHESIS_OUTPUT_DIR / f\"{verilog_file.stem}_synth.json\"\n",
    "        p = os.environ.copy()\n",
    "        p[\"OUT_JSON_PATH\"] = str(synthesis_out)\n",
    "        p[\"TOP_MODULE\"] = verilog_file.stem\n",
    "        synthesis_result = subprocess.run(synthesis_cmd, capture_output=True, text=True, timeout=300, env=p)\n",
    "        synthesis_runtime = time.time() - start_time\n",
    "        \n",
    "        synthesis_success = synthesis_result.returncode == 0\n",
    "        error_message = synthesis_result.stderr if not synthesis_success else None\n",
    "\n",
    "        # Check for invalid synthesis cells\n",
    "        if synthesis_success and re.findall(r\"\\\"type\\\": \\\"(\\$.*?)\\\"\", synthesis_result.stdout):\n",
    "            r = re.findall(r\"\\\"type\\\": \\\"(\\$.*?)\\\"\", synthesis_result.stdout)\n",
    "            synthesis_success = False\n",
    "            error_message = f\"Invalid synthesis result, found cell: {set(r)}\"\n",
    "        \n",
    "        return SynthesisResult(\n",
    "            success=synthesis_success,\n",
    "            runtime=synthesis_runtime,\n",
    "            verilog_file=verilog_file,\n",
    "            synthesis_out_file=synthesis_out if synthesis_success else None,\n",
    "            error_message=error_message,\n",
    "            full_stdout=synthesis_result.stdout,\n",
    "            full_stderr=synthesis_result.stderr\n",
    "        )\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return SynthesisResult(\n",
    "            success=False,\n",
    "            runtime=time.time() - start_time,\n",
    "            verilog_file=verilog_file,\n",
    "            synthesis_out_file=None,\n",
    "            error_message=\"Synthesis timed out\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return SynthesisResult(\n",
    "            success=False,\n",
    "            runtime=time.time() - start_time,\n",
    "            verilog_file=verilog_file,\n",
    "            synthesis_out_file=None,\n",
    "            error_message=f\"Synthesis error: {str(e)}\"\n",
    "        )\n",
    "\n",
    "def run_parameter_sweep(synthesis_result: SynthesisResult, custom_params: dict = None) -> List[ParameterResult]:\n",
    "    \"\"\"Run NextPNR parameter sweep only - returns list of ParameterResult directly\"\"\"\n",
    "    if not synthesis_result.success or not synthesis_result.synthesis_out_file:\n",
    "        return []\n",
    "    \n",
    "    synthesis_json_path = Path(synthesis_result.synthesis_out_file)\n",
    "    \n",
    "    # Check if synthesis JSON exists\n",
    "    if not synthesis_json_path.exists():\n",
    "        return [ParameterResult(\n",
    "            config_id=0,\n",
    "            success=False,\n",
    "            runtime=0.0,\n",
    "            failure_type=FailureType.SYNTHESIS,\n",
    "            error_message=f\"Synthesis JSON not found: {synthesis_json_path}\"\n",
    "        )]\n",
    "    \n",
    "    PNR_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    out_json_path = PNR_OUTPUT_DIR / f\"{synthesis_json_path.stem}_pnr.json\"\n",
    "    out_fasm_path = PNR_OUTPUT_DIR / f\"{synthesis_json_path.stem}.fasm\"\n",
    "    parameter_results = []\n",
    "\n",
    "        # Use custom parameters or single default configuration\n",
    "    if custom_params:\n",
    "        conn_factor = custom_params.get('connectivity_factor', 0.0)\n",
    "        cong_factor = custom_params.get('congestion_factor', 0.0)\n",
    "        config_grid = [(conn_factor, cong_factor)]\n",
    "    else:\n",
    "        config_grid = grid  # Single default configuration\n",
    "    \n",
    "    for i, (conn_factor, cong_factor) in enumerate(config_grid):\n",
    "        param_start_time = time.time()\n",
    "        \n",
    "        # Run nextpnr with these parameters\n",
    "        try:\n",
    "            nextpnr_cmd = [\n",
    "                \"nextpnr-himbaechel\",\n",
    "                \"--chipdb\", str(CHIPDB_PATH),\n",
    "                \"--device\", \"FABulous\",\n",
    "                \"--json\", str(synthesis_json_path),\n",
    "                \"--write\", str(out_json_path),\n",
    "                \"-o\", f\"constrain-pair={CONSTRAIN_PAIR}\",\n",
    "                \"-o\", f\"fasm={out_fasm_path}\",  # Uncomment if FASM output needed\n",
    "                \"-o\", f\"fdc={FDC_PATH}\",\n",
    "                \"--placer-heap-seed-placement-strategy\", \"graph_grid\",\n",
    "                \"--placer-heap-beta\", str(BETA_VALUE),\n",
    "                \"--placer-heap-arch-connectivity-factor\", str(conn_factor),\n",
    "                \"--placer-heap-congestion-aware-factor\", str(cong_factor),\n",
    "                \"-o\", f\"placeTrial={PLACE_TRIALS}\",\n",
    "                \"--router1-timeout\", str(ROUTER_TIMEOUT)\n",
    "            ]\n",
    "            \n",
    "            result = subprocess.run(nextpnr_cmd, capture_output=True, text=True, timeout=600)\n",
    "            param_runtime = time.time() - param_start_time\n",
    "            \n",
    "            param_success = result.returncode == 0\n",
    "            failure_type = FailureType.NONE if param_success else FailureType.ROUTING\n",
    "            \n",
    "            # Simple log analysis\n",
    "            placement_success = \"Final Placement\" in result.stdout\n",
    "            routing_failed = \"ERROR: Max iteration count reached\" in result.stdout\n",
    "            \n",
    "            if not param_success:\n",
    "                if not placement_success:\n",
    "                    failure_type = FailureType.PLACEMENT\n",
    "                elif routing_failed:\n",
    "                    failure_type = FailureType.ROUTING\n",
    "                else:\n",
    "                    failure_type = FailureType.UNKNOWN\n",
    "            \n",
    "            param_result = ParameterResult(\n",
    "                config_id=i,\n",
    "                success=param_success,\n",
    "                runtime=param_runtime,\n",
    "                parameters={'connectivity_factor': conn_factor, 'congestion_factor': cong_factor},\n",
    "                placement_info={'placement_success': placement_success},\n",
    "                routing_info={'routing_failed': routing_failed},\n",
    "                failure_type=failure_type,\n",
    "                full_stderr= result.stderr,\n",
    "                full_stdout=result.stdout,\n",
    "                error_message=result.stderr if not param_success else None\n",
    "            )\n",
    "            \n",
    "            parameter_results.append(param_result)\n",
    "            \n",
    "        except subprocess.TimeoutExpired:\n",
    "            param_runtime = time.time() - param_start_time\n",
    "            parameter_results.append(ParameterResult(\n",
    "                config_id=i,\n",
    "                success=False,\n",
    "                runtime=param_runtime,\n",
    "                parameters={'connectivity_factor': conn_factor, 'congestion_factor': cong_factor},\n",
    "                placement_info={},\n",
    "                routing_info={},\n",
    "                failure_type=FailureType.UNKNOWN,\n",
    "                error_message='Parameter run timed out'\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            param_runtime = time.time() - param_start_time\n",
    "            parameter_results.append(ParameterResult(\n",
    "                config_id=i,\n",
    "                success=False,\n",
    "                runtime=param_runtime,\n",
    "                parameters={'connectivity_factor': conn_factor, 'congestion_factor': cong_factor},\n",
    "                placement_info={},\n",
    "                routing_info={},\n",
    "                failure_type=FailureType.UNKNOWN,\n",
    "                error_message=str(e)\n",
    "            ))\n",
    "    \n",
    "    return parameter_results\n",
    "\n",
    "def run_complete_pipeline(mlir_file: Path) -> CompletePipelineResult:\n",
    "    \"\"\"\n",
    "    Run the complete MLIR to hardware pipeline:\n",
    "    MLIR -> Verilog -> Synthesis -> Parameter Sweep\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Ensure all stage directories exist\n",
    "    MLIR_OPTIMIZED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MLIR_EXTRACTED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    FUTIL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    VERILOG_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    SYNTHESIS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    PNR_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Stage 1: Verilog Generation Pipeline\n",
    "    verilog_result = run_verilog_pipeline(mlir_file)\n",
    "    \n",
    "    # Stage 2: Synthesis (if Verilog generation succeeded)\n",
    "    synthesis_results = []\n",
    "    if verilog_result.success and verilog_result.verilog_files:\n",
    "        for verilog_file in verilog_result.verilog_files:\n",
    "            synthesis_results.append(run_synthesis(Path(verilog_file)))\n",
    "    \n",
    "    # Stage 3: Parameter Sweep (if any synthesis succeeded)\n",
    "    parameter_results = []\n",
    "    for synthesis_result in synthesis_results:\n",
    "        if synthesis_result.success:\n",
    "            parameter_results.extend(run_parameter_sweep(synthesis_result))\n",
    "\n",
    "    # Determine overall success and primary failure type\n",
    "    overall_success = False\n",
    "    primary_failure_type = FailureType.NONE\n",
    "    \n",
    "    if not verilog_result.success:\n",
    "        primary_failure_type = verilog_result.failure_stage or FailureType.VERILOG_GENERATION\n",
    "    elif not synthesis_results or not any(s.success for s in synthesis_results):\n",
    "        primary_failure_type = FailureType.SYNTHESIS\n",
    "    elif not parameter_results or not any(p.success for p in parameter_results):\n",
    "        # Find most common parameter failure type\n",
    "        if parameter_results:\n",
    "            failure_types = [p.failure_type for p in parameter_results if p.failure_type]\n",
    "            if failure_types:\n",
    "                primary_failure_type = max(set(failure_types), key=failure_types.count)\n",
    "            else:\n",
    "                primary_failure_type = FailureType.UNKNOWN\n",
    "        else:\n",
    "            primary_failure_type = FailureType.UNKNOWN\n",
    "    else:\n",
    "        overall_success = True\n",
    "        primary_failure_type = FailureType.NONE\n",
    "    \n",
    "    # Create and return complete result\n",
    "    return CompletePipelineResult(\n",
    "        source_file=mlir_file,\n",
    "        overall_success=overall_success,\n",
    "        total_runtime=time.time() - start_time,\n",
    "        verilog_pipeline=verilog_result,\n",
    "        synthesis_results=synthesis_results,\n",
    "        parameter_results=parameter_results,\n",
    "        primary_failure_type=primary_failure_type\n",
    "    )\n",
    "\n",
    "def process_mlir_benchmarks(mlir_files: List[Path]) -> List[CompletePipelineResult]:\n",
    "    \"\"\"Process multiple MLIR files through the complete pipeline\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(mlir_files)} MLIR files through complete pipeline...\")\n",
    "    print(\"Stage directories:\")\n",
    "    print(f\"  01_optimized_mlir: {MLIR_OPTIMIZED_DIR}\")\n",
    "    print(f\"  02_extracted_mlir: {MLIR_EXTRACTED_DIR}\")\n",
    "    print(f\"  03_futil: {FUTIL_OUTPUT_DIR}\")\n",
    "    print(f\"  04_verilog: {VERILOG_OUTPUT_DIR}\")\n",
    "    print(f\"  05_synthesis: {SYNTHESIS_OUTPUT_DIR}\")\n",
    "    print(f\"  06_PNR: {PNR_OUTPUT_DIR}\")\n",
    "\n",
    "    for i, mlir_file in enumerate(mlir_files):\n",
    "        print(f\"\\n[{i+1}/{len(mlir_files)}] Processing: {Path(mlir_file).name}\")\n",
    "        \n",
    "        try:\n",
    "            result = run_complete_pipeline(mlir_file)\n",
    "            results.append(result)\n",
    "            \n",
    "            status = \"âœ“ SUCCESS\" if result.overall_success else f\"âœ— FAILED ({result.primary_failure_type.value})\"\n",
    "            print(f\"  Result: {status} | Runtime: {result.total_runtime:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            # Create failed result with proper structure\n",
    "            failed_verilog_result = VerilogPipelineResult(\n",
    "                source_file=mlir_file,\n",
    "                success=False,\n",
    "                runtime=0.0,\n",
    "                failure_stage=FailureType.UNKNOWN,\n",
    "                error_message=str(e)\n",
    "            )\n",
    "            \n",
    "            failed_result = CompletePipelineResult(\n",
    "                source_file=mlir_file,\n",
    "                overall_success=False,\n",
    "                total_runtime=0.0,\n",
    "                verilog_pipeline=failed_verilog_result,\n",
    "                synthesis_results=[],\n",
    "                parameter_results=[],\n",
    "                primary_failure_type=FailureType.UNKNOWN\n",
    "            )\n",
    "            results.append(failed_result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6838d7",
   "metadata": {},
   "source": [
    "## 7. Interactive Dashboard\n",
    "\n",
    "Interactive dashboard for exploring results across different Verilog files with combined analysis and file-specific views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thh4i34q6tg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5185e44e33f349eb8663223336104df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>ðŸ§ª Single MLIR File Pipeline Experiment</h3>'), HBox(children=(Dropdown(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Interactive Dashboard loaded successfully!\n",
      "ðŸ“ Select an MLIR file and click 'Run Pipeline' to start.\n",
      "ðŸ“„ File dropdown is now sorted alphabetically.\n"
     ]
    }
   ],
   "source": [
    "# Single MLIR File Pipeline Runner with Interactive Dashboard\n",
    "# Fixed widget system to prevent multiple executions\n",
    "\n",
    "def make_clickable_path(file_path: Path) -> str:\n",
    "    \"\"\"Convert a file path into a clickable terminal hyperlink using OSC 8 sequences\"\"\"\n",
    "    if not file_path:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to absolute path\n",
    "    abs_path = file_path.resolve()\n",
    "    \n",
    "    # Create OSC 8 hyperlink: \\033]8;;file://path\\033\\\\text\\033]8;;\\033\\\\\n",
    "    # This creates a clickable link in terminals that support OSC 8\n",
    "    return f\"file://{abs_path}\"\n",
    "\n",
    "def make_clickable_dir(dir_path: Path) -> str:\n",
    "    \"\"\"Convert a directory path into a clickable terminal hyperlink\"\"\"\n",
    "    if not dir_path:\n",
    "        return \"\"\n",
    "    \n",
    "    abs_path = dir_path.resolve()\n",
    "    return f\"file://{abs_path}\"\n",
    "\n",
    "def clean_log_content(log_content: str) -> str:\n",
    "    \"\"\"Remove ANSI escape codes, color codes, and other rich formatting from log content\"\"\"\n",
    "    import re\n",
    "    \n",
    "    if not log_content:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove ANSI escape sequences (colors, cursor movements, etc.)\n",
    "    # Pattern matches: ESC[ followed by any number of parameters and a final character\n",
    "    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "    cleaned = ansi_escape.sub('', log_content)\n",
    "    \n",
    "    # Remove other common escape sequences\n",
    "    # Remove carriage returns that don't have newlines (progress indicators)\n",
    "    cleaned = re.sub(r'\\r(?!\\n)', '\\n', cleaned)\n",
    "    \n",
    "    # Remove excessive whitespace but preserve structure\n",
    "    cleaned = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', cleaned)\n",
    "    \n",
    "    # Remove common progress indicators and spinner characters\n",
    "    cleaned = re.sub(r'[â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â |\\-\\\\/]', '', cleaned)\n",
    "    \n",
    "    # Remove excessive spaces (more than 4 consecutive spaces become 4)\n",
    "    cleaned = re.sub(r' {5,}', '    ', cleaned)\n",
    "    \n",
    "    # Clean up any remaining control characters except tabs and newlines\n",
    "    cleaned = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', cleaned)\n",
    "    \n",
    "    # Remove lines that are just whitespace or repetitive characters\n",
    "    lines = cleaned.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        # Skip lines that are just repeated characters (like ===== or -----)\n",
    "        if line.strip() and not re.match(r'^[=\\-_*#]{10,}$', line.strip()):\n",
    "            cleaned_lines.append(line)\n",
    "        elif not line.strip():\n",
    "            # Keep empty lines but limit consecutive empty lines\n",
    "            if not (cleaned_lines and cleaned_lines[-1] == ''):\n",
    "                cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines).strip()\n",
    "\n",
    "def display_scrollable_log(log_content: str, title: str, max_height: str = \"300px\"):\n",
    "    \"\"\"Display log content in a scrollable text area with rich formatting removed\"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "    import html\n",
    "    \n",
    "    # Clean the log content first\n",
    "    cleaned_content = clean_log_content(log_content)\n",
    "    \n",
    "    # Add summary info if content was cleaned\n",
    "    original_lines = len(log_content.split('\\n')) if log_content else 0\n",
    "    cleaned_lines = len(cleaned_content.split('\\n')) if cleaned_content else 0\n",
    "    \n",
    "    # Escape HTML characters\n",
    "    escaped_content = html.escape(cleaned_content)\n",
    "    \n",
    "    # Add info about filtering if significant content was removed\n",
    "    filter_info = \"\"\n",
    "    if original_lines > cleaned_lines + 10:  # Significant filtering occurred\n",
    "        filter_info = f\"\"\"\n",
    "        <div style=\"font-size: 11px; color: #666; padding: 4px 8px; background-color: #f0f0f0; border-bottom: 1px solid #ddd;\">\n",
    "            ðŸ“‹ Log filtered: {original_lines} â†’ {cleaned_lines} lines (removed formatting/progress indicators)\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Create scrollable HTML\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"border: 1px solid #ccc; border-radius: 5px; margin: 10px 0;\">\n",
    "        <div style=\"background-color: #f5f5f5; padding: 8px; border-bottom: 1px solid #ccc; font-weight: bold;\">\n",
    "            {title}\n",
    "        </div>\n",
    "        {filter_info}\n",
    "        <div style=\"max-height: {max_height}; overflow-y: auto; padding: 10px; font-family: 'Courier New', monospace; font-size: 12px; background-color: #fafafa; line-height: 1.3;\">\n",
    "            <pre style=\"margin: 0; white-space: pre-wrap; word-wrap: break-word;\">{escaped_content}</pre>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(html_content))\n",
    "\n",
    "\n",
    "def run_single_file_experiment(mlir_file_path: str, verbose: bool = True, \n",
    "                              custom_params: dict = None):\n",
    "    \"\"\"\n",
    "    Run the complete pipeline on a single MLIR file with detailed output for each stage.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mlir_file_path : str\n",
    "        Path to the MLIR file to process\n",
    "    verbose : bool\n",
    "        Whether to show detailed output for each stage\n",
    "    custom_params : dict\n",
    "        Custom parameter configuration for NextPNR sweep\n",
    "        Format: {'connectivity_factor': float, 'congestion_factor': float}\n",
    "        If None, uses a single default configuration (0.0, 0.0)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    CompletePipelineResult\n",
    "        Complete pipeline result with all stage information\n",
    "    \"\"\"\n",
    "    mlir_path = Path(mlir_file_path)\n",
    "    \n",
    "    if not mlir_path.exists():\n",
    "        print(f\"âŒ ERROR: MLIR file not found: {mlir_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ðŸš€ Starting Complete Pipeline Experiment\")\n",
    "    print(f\"ðŸ“ Input File: {make_clickable_path(mlir_path)}\")\n",
    "    print(f\"ðŸ“‚ Source Path: {make_clickable_path(mlir_path)}\")\n",
    "    print(f\"â° Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    if custom_params:\n",
    "        print(f\"ðŸŽ›ï¸  Custom Parameters: connectivity={custom_params.get('connectivity_factor', 'N/A')}, congestion={custom_params.get('congestion_factor', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"ðŸŽ›ï¸  Using Default Single Configuration: connectivity=0.0, congestion=0.0\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Stage 1: Verilog Generation Pipeline\n",
    "    verilog_result = run_verilog_pipeline(mlir_path)\n",
    "    \n",
    "    # Stage 2: Synthesis (if Verilog generation succeeded)\n",
    "    synthesis_results = []\n",
    "    if verilog_result.success and verilog_result.verilog_files:\n",
    "        for verilog_file in verilog_result.verilog_files:\n",
    "            synthesis_results.append(run_synthesis(Path(verilog_file)))\n",
    "    \n",
    "    # Stage 3: Parameter Sweep (if any synthesis succeeded)\n",
    "    parameter_results = []\n",
    "    for synthesis_result in synthesis_results:\n",
    "        if synthesis_result.success:\n",
    "            parameter_results.extend(run_parameter_sweep(synthesis_result, custom_params))\n",
    "\n",
    "    # Determine overall success and primary failure type\n",
    "    overall_success = False\n",
    "    primary_failure_type = FailureType.NONE\n",
    "    \n",
    "    if not verilog_result.success:\n",
    "        primary_failure_type = verilog_result.failure_stage or FailureType.VERILOG_GENERATION\n",
    "    elif not synthesis_results or not any(s.success for s in synthesis_results):\n",
    "        primary_failure_type = FailureType.SYNTHESIS\n",
    "    elif not parameter_results or not any(p.success for p in parameter_results):\n",
    "        # Find most common parameter failure type\n",
    "        if parameter_results:\n",
    "            failure_types = [p.failure_type for p in parameter_results if p.failure_type]\n",
    "            if failure_types:\n",
    "                primary_failure_type = max(set(failure_types), key=failure_types.count)\n",
    "            else:\n",
    "                primary_failure_type = FailureType.UNKNOWN\n",
    "        else:\n",
    "            primary_failure_type = FailureType.UNKNOWN\n",
    "    else:\n",
    "        overall_success = True\n",
    "        primary_failure_type = FailureType.NONE\n",
    "    \n",
    "    # Create result object\n",
    "    result = CompletePipelineResult(\n",
    "        source_file=mlir_path,\n",
    "        overall_success=overall_success,\n",
    "        total_runtime=time.time() - start_time,\n",
    "        verilog_pipeline=verilog_result,\n",
    "        synthesis_results=synthesis_results,\n",
    "        parameter_results=parameter_results,\n",
    "        primary_failure_type=primary_failure_type\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Display detailed results\n",
    "    print(f\"\\nðŸ Pipeline Completed in {total_time:.2f}s\")\n",
    "    print(f\"ðŸ“Š Overall Success: {'âœ… PASS' if result.overall_success else 'âŒ FAIL'}\")\n",
    "    print(f\"ðŸŽ¯ Primary Failure: {result.primary_failure_type.value}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # === STAGE 1: VERILOG PIPELINE ===\n",
    "    print(\"\\nðŸ“‹ STAGE 1: VERILOG GENERATION PIPELINE\")\n",
    "    print(f\"â±ï¸  Total Stage Time: {result.verilog_pipeline.runtime:.2f}s\")\n",
    "    print(f\"âœ… Stage Success: {'PASS' if result.verilog_pipeline.success else 'FAIL'}\")\n",
    "    \n",
    "    if verbose:\n",
    "        vp = result.verilog_pipeline\n",
    "        print(\"\\n  ðŸ”§ Sub-Stage 1.1: MLIR Optimization\")\n",
    "        print(f\"     â±ï¸  Time: {vp.mlir_optimization_time:.2f}s\")\n",
    "        print(f\"     ðŸ“„ Input: {make_clickable_path(mlir_path)}\")\n",
    "        if vp.optimized_mlir_file:\n",
    "            print(f\"     ðŸ“„ Output: {make_clickable_path(vp.optimized_mlir_file)}\")\n",
    "        \n",
    "        print(\"\\n  ðŸ”„ Sub-Stage 1.2: Loop Extraction\")\n",
    "        print(f\"     â±ï¸  Time: {vp.loop_extraction_time:.2f}s\")\n",
    "        if vp.function_extracted is not None:\n",
    "            print(f\"     ðŸ“Š Extracted Files: {len(vp.function_extracted)}\")\n",
    "        if vp.function_extracted:\n",
    "            for i, func in enumerate(vp.function_extracted):\n",
    "                print(f\"     ðŸ“‚ Function {i+1}: {make_clickable_path(func)}\")\n",
    "        \n",
    "        print(\"\\n  âš™ï¸  Sub-Stage 1.3: FUTIL Generation\")\n",
    "        print(f\"     â±ï¸  Time: {vp.futil_generation_time:.2f}s\")\n",
    "        if vp.futil_files:\n",
    "            for i, futil in enumerate(vp.futil_files):\n",
    "                print(f\"     ðŸ“„ FUTIL File {i+1}: {make_clickable_path(futil)}\")\n",
    "        \n",
    "        print(\"\\n  ðŸ“¦ Final Verilog Output:\")\n",
    "        if vp.verilog_files:\n",
    "            print(f\"     ðŸ“Š Generated Files: {len(vp.verilog_files)}\")\n",
    "            for i, vf in enumerate(vp.verilog_files):\n",
    "                print(f\"       {i+1}. {make_clickable_path(vf)}\")\n",
    "        else:\n",
    "            print(\"     âŒ No Verilog files generated\")\n",
    "        \n",
    "        if not vp.success:\n",
    "            print(\"\\n  âŒ FAILURE DETAILS:\")\n",
    "            print(f\"     ðŸŽ¯ Failed Stage: {vp.failure_stage.value if vp.failure_stage else 'unknown'}\")\n",
    "            if vp.error_message:\n",
    "                print(f\"     ðŸ’¬ Error: {vp.error_message}\")\n",
    "    \n",
    "    # === STAGE 2: SYNTHESIS ===\n",
    "    print(\"\\nðŸ“‹ STAGE 2: SYNTHESIS (FABulous)\")\n",
    "    if result.synthesis_results:\n",
    "        successful_synthesis = sum(1 for s in result.synthesis_results if s.success)\n",
    "        total_synthesis_time = sum(s.runtime for s in result.synthesis_results)\n",
    "        print(f\"â±ï¸  Total Stage Time: {total_synthesis_time:.2f}s\")\n",
    "        print(f\"âœ… Stage Success: {successful_synthesis}/{len(result.synthesis_results)} operations successful\")\n",
    "        \n",
    "        if verbose:\n",
    "            for i, synth in enumerate(result.synthesis_results):\n",
    "                print(f\"\\n  ðŸ”¨ Synthesis Operation {i+1}:\")\n",
    "                print(f\"     âœ… Success: {'PASS' if synth.success else 'FAIL'}\")\n",
    "                print(f\"     â±ï¸  Runtime: {synth.runtime:.2f}s\")\n",
    "                if synth.verilog_file:\n",
    "                    print(f\"     ðŸ“„ Input Verilog: {make_clickable_path(synth.verilog_file)}\")\n",
    "                if synth.synthesis_out_file:\n",
    "                    print(f\"     ðŸ“„ Output JSON: {make_clickable_path(synth.synthesis_out_file)}\")\n",
    "                if synth.error_message:\n",
    "                    print(f\"     âŒ Error: {synth.error_message}\")\n",
    "                \n",
    "                # Display scrollable logs directly in output\n",
    "                if hasattr(synth, 'full_stdout') and synth.full_stdout:\n",
    "                    print(f\"\\n     ðŸ“œ Synthesis {i+1} Output Log:\")\n",
    "                    display_scrollable_log(synth.full_stdout, f\"Synthesis {i+1} Output Log\", max_height=\"200px\")\n",
    "                    \n",
    "                if hasattr(synth, 'full_stderr') and synth.full_stderr:\n",
    "                    print(f\"\\n     ðŸ“œ Synthesis {i+1} Error Log:\")\n",
    "                    display_scrollable_log(synth.full_stderr, f\"Synthesis {i+1} Error Log\", max_height=\"200px\")\n",
    "    else:\n",
    "        print(\"âŒ No synthesis operations performed\")\n",
    "\n",
    "    # === STAGE 3: PARAMETER SWEEP ===\n",
    "    print(\"\\nðŸ“‹ STAGE 3: PARAMETER SWEEP (NextPNR)\")\n",
    "    if result.parameter_results:\n",
    "        successful_params = sum(1 for p in result.parameter_results if p.success)\n",
    "        total_param_time = sum(p.runtime for p in result.parameter_results)\n",
    "        avg_param_time = total_param_time / len(result.parameter_results)\n",
    "        \n",
    "        print(f\"â±ï¸  Total Stage Time: {total_param_time:.2f}s\")\n",
    "        print(f\"â±ï¸  Average Config Time: {avg_param_time:.2f}s\")\n",
    "        print(f\"âœ… Stage Success: {successful_params}/{len(result.parameter_results)} configurations successful\")\n",
    "        print(f\"ðŸ“Š Success Rate: {successful_params/len(result.parameter_results)*100:.1f}%\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n  ðŸ“ˆ Parameter Configuration Results:\")\n",
    "            failure_types = {}\n",
    "            for param in result.parameter_results:\n",
    "                conn_factor = param.parameters.get('connectivity_factor', 'N/A')\n",
    "                cong_factor = param.parameters.get('congestion_factor', 'N/A')\n",
    "                \n",
    "                if param.success:\n",
    "                    print(f\"     âœ… Config {param.config_id}: conn={conn_factor}, cong={cong_factor}, time={param.runtime:.2f}s\")\n",
    "                else:\n",
    "                    failure_type = param.failure_type.value if param.failure_type else 'unknown'\n",
    "                    failure_types[failure_type] = failure_types.get(failure_type, 0) + 1\n",
    "                    print(f\"     âŒ Config {param.config_id}: conn={conn_factor}, cong={cong_factor}\")\n",
    "                    print(f\"        ðŸŽ¯ Failure: {failure_type}, Time: {param.runtime:.2f}s\")\n",
    "                    if param.error_message:\n",
    "                        error_preview = param.error_message[:100] + \"...\" if len(param.error_message) > 100 else param.error_message\n",
    "                        print(f\"        ðŸ’¬ Error: {error_preview}\")\n",
    "                \n",
    "                # Display scrollable logs directly in output for each config\n",
    "                if hasattr(param, 'full_stdout') and param.full_stdout:\n",
    "                    print(f\"\\n     ðŸ“œ NextPNR Config {param.config_id} Output Log:\")\n",
    "                    display_scrollable_log(param.full_stdout, f\"NextPNR Config {param.config_id} Output Log\", max_height=\"300px\")\n",
    "                    \n",
    "                if hasattr(param, 'full_stderr') and param.full_stderr:\n",
    "                    print(f\"\\n     ðŸ“œ NextPNR Config {param.config_id} Error Log:\")\n",
    "                    display_scrollable_log(param.full_stderr, f\"NextPNR Config {param.config_id} Error Log\", max_height=\"200px\")\n",
    "            \n",
    "            if failure_types:\n",
    "                print(\"\\n  âŒ Parameter Failure Summary:\")\n",
    "                for failure_type, count in failure_types.items():\n",
    "                    print(f\"     ðŸŽ¯ {failure_type}: {count} configurations\")\n",
    "    else:\n",
    "        print(\"âŒ No parameter sweep performed\")\n",
    "    \n",
    "    # === SUMMARY SECTION ===\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ðŸ“Š EXPERIMENT SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ðŸ“ File: {make_clickable_path(mlir_path)}\")\n",
    "    print(f\"â° Total Runtime: {result.total_runtime:.2f}s\")\n",
    "    print(f\"ðŸŽ¯ Overall Result: {'ðŸŽ‰ SUCCESS' if result.overall_success else 'ðŸ’¥ FAILED'}\")\n",
    "    \n",
    "    if not result.overall_success:\n",
    "        print(f\"âŒ Primary Failure: {result.primary_failure_type.value}\")\n",
    "    \n",
    "    # Performance breakdown\n",
    "    print(\"\\nâ±ï¸  Stage Performance Breakdown:\")\n",
    "    print(f\"   Verilog Pipeline:  {result.verilog_pipeline.runtime:6.2f}s ({result.verilog_pipeline.runtime/result.total_runtime*100:4.1f}%)\")\n",
    "    if result.synthesis_results:\n",
    "        synth_time = sum(s.runtime for s in result.synthesis_results)\n",
    "        print(f\"   Synthesis:         {synth_time:6.2f}s ({synth_time/result.total_runtime*100:4.1f}%)\")\n",
    "    if result.parameter_results:\n",
    "        param_time = sum(p.runtime for p in result.parameter_results)\n",
    "        print(f\"   Parameter Sweep:   {param_time:6.2f}s ({param_time/result.total_runtime*100:4.1f}%)\")\n",
    "    \n",
    "    # Resource utilization\n",
    "    print(\"\\nðŸ“¦ Resource Utilization:\")\n",
    "    if result.verilog_pipeline.verilog_files:\n",
    "        print(f\"   Verilog Files Generated: {len(result.verilog_pipeline.verilog_files)}\")\n",
    "    if result.synthesis_results:\n",
    "        print(f\"   Synthesis Operations: {len(result.synthesis_results)}\")\n",
    "    if result.parameter_results:\n",
    "        print(f\"   Parameter Configurations: {len(result.parameter_results)}\")\n",
    "    \n",
    "    # Stage directories info with clickable links\n",
    "    print(\"\\nðŸ“‚ Output Files Located In:\")\n",
    "    print(f\"   MLIR Optimized:    {make_clickable_dir(MLIR_OPTIMIZED_DIR)}\")\n",
    "    print(f\"   MLIR Extracted:    {make_clickable_dir(MLIR_EXTRACTED_DIR)}\")\n",
    "    print(f\"   FUTIL Files:       {make_clickable_dir(FUTIL_OUTPUT_DIR)}\")\n",
    "    print(f\"   Verilog Files:     {make_clickable_dir(VERILOG_OUTPUT_DIR)}\")\n",
    "    print(f\"   Synthesis Results: {make_clickable_dir(SYNTHESIS_OUTPUT_DIR)}\")\n",
    "    print(f\"   PnR Results:       {make_clickable_dir(PNR_OUTPUT_DIR)}\")\n",
    "    \n",
    "    print(f\"\\nðŸ Experiment completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# PROPER WIDGET SYSTEM with execution control\n",
    "class ExperimentController:\n",
    "    \"\"\"Controller class to manage experiment execution and prevent multiple runs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.is_running = False\n",
    "        self.widgets = {}\n",
    "        \n",
    "    def create_widgets(self):\n",
    "        \"\"\"Create all widgets fresh\"\"\"\n",
    "        # Get available MLIR files\n",
    "        available_files = list(Path(BENCHMARK_ROOT_DIR).glob(\"*.mlir\"))\n",
    "        if not available_files:\n",
    "            print(f\"âŒ No MLIR files found in {BENCHMARK_ROOT_DIR}\")\n",
    "            return None\n",
    "            \n",
    "        # Sort files alphabetically by name (case-insensitive)\n",
    "        available_files.sort(key=lambda f: f.name.lower())\n",
    "        file_options = [(f.name, str(f)) for f in available_files]\n",
    "        \n",
    "        # Create widgets\n",
    "        self.widgets['file_selector'] = widgets.Dropdown(\n",
    "            options=file_options,\n",
    "            description='MLIR File:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        self.widgets['verbose_checkbox'] = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Show detailed output',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.widgets['use_grid_checkbox'] = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Use grid',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.widgets['connectivity_slider'] = widgets.FloatSlider(\n",
    "            value=0.0,\n",
    "            min=0.0,\n",
    "            max=2.0,\n",
    "            step=0.1,\n",
    "            description='Connectivity:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        self.widgets['congestion_slider'] = widgets.FloatSlider(\n",
    "            value=0.0,\n",
    "            min=0.0,\n",
    "            max=2.0,\n",
    "            step=0.1,\n",
    "            description='Congestion:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        self.widgets['run_button'] = widgets.Button(\n",
    "            description='ðŸš€ Run Pipeline',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        \n",
    "        self.widgets['clear_button'] = widgets.Button(\n",
    "            description='ðŸ§¹ Clear Output',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        \n",
    "        self.widgets['output_area'] = widgets.Output()\n",
    "        \n",
    "        # Set up interactions\n",
    "        self.widgets['use_grid_checkbox'].observe(self._toggle_custom_params, names='value')\n",
    "        self.widgets['run_button'].on_click(self._on_run_button_click)\n",
    "        self.widgets['clear_button'].on_click(self._on_clear_button_click)\n",
    "        \n",
    "        return self._create_layout()\n",
    "    \n",
    "    def _toggle_custom_params(self, change):\n",
    "        \"\"\"Toggle custom parameter sliders\"\"\"\n",
    "        # enabled = change['new']\n",
    "        self.widgets['connectivity_slider'].disabled = change['new']\n",
    "        self.widgets['congestion_slider'].disabled = change['new']\n",
    "    \n",
    "    def _on_run_button_click(self, button):\n",
    "        \"\"\"Handle run button click with execution control\"\"\"\n",
    "        if self.is_running:\n",
    "            with self.widgets['output_area']:\n",
    "                print(\"âš ï¸ Experiment already running! Please wait for completion.\")\n",
    "            return\n",
    "        \n",
    "        self.is_running = True\n",
    "        button.disabled = True\n",
    "        button.description = 'â³ Running...'\n",
    "        \n",
    "        try:\n",
    "            with self.widgets['output_area']:\n",
    "                self.widgets['output_area'].clear_output(wait=True)\n",
    "                \n",
    "                # Get parameters\n",
    "                selected_file = self.widgets['file_selector'].value\n",
    "                verbose = self.widgets['verbose_checkbox'].value\n",
    "                use_grid_params = self.widgets['use_grid_checkbox'].value\n",
    "                \n",
    "                custom_params = None\n",
    "                if not use_grid_params:\n",
    "                    custom_params = {\n",
    "                        'connectivity_factor': self.widgets['connectivity_slider'].value,\n",
    "                        'congestion_factor': self.widgets['congestion_slider'].value\n",
    "                    }\n",
    "                \n",
    "                # Run experiment\n",
    "                try:\n",
    "                    result = run_single_file_experiment(selected_file, verbose, custom_params)\n",
    "                    if result:\n",
    "                        print(\"\\nâœ… Experiment completed successfully!\")\n",
    "                    else:\n",
    "                        print(\"\\nâŒ Experiment failed to start!\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nðŸ’¥ Experiment crashed with error: {str(e)}\")\n",
    "                    import traceback\n",
    "                    print(\"Full traceback:\")\n",
    "                    print(traceback.format_exc())\n",
    "        finally:\n",
    "            # Always reset state\n",
    "            self.is_running = False\n",
    "            button.disabled = False\n",
    "            button.description = 'ðŸš€ Run Pipeline'\n",
    "    \n",
    "    def _on_clear_button_click(self, button):\n",
    "        \"\"\"Handle clear button click\"\"\"\n",
    "        self.widgets['output_area'].clear_output(wait=True)\n",
    "    \n",
    "    def _create_layout(self):\n",
    "        \"\"\"Create the widget layout\"\"\"\n",
    "        main_controls = widgets.HBox([\n",
    "            self.widgets['file_selector'],\n",
    "            self.widgets['verbose_checkbox'],\n",
    "            self.widgets['run_button'],\n",
    "            self.widgets['clear_button']\n",
    "        ])\n",
    "        \n",
    "        param_controls = widgets.VBox([\n",
    "            self.widgets['use_grid_checkbox'],\n",
    "            widgets.HBox([\n",
    "                self.widgets['connectivity_slider'], \n",
    "                self.widgets['congestion_slider']\n",
    "            ])\n",
    "        ])\n",
    "        \n",
    "        return widgets.VBox([\n",
    "            widgets.HTML(\"<h3>ðŸ§ª Single MLIR File Pipeline Experiment</h3>\"),\n",
    "            main_controls,\n",
    "            widgets.HTML(\"<h4>ðŸŽ›ï¸ Parameter Configuration</h4>\"),\n",
    "            param_controls,\n",
    "            widgets.HTML(\"<h4>ðŸ“‹ Results</h4>\"),\n",
    "            self.widgets['output_area']\n",
    "        ])\n",
    "\n",
    "# CREATE AND DISPLAY THE DASHBOARD\n",
    "try:\n",
    "    # Clean up any existing global controller\n",
    "    if 'experiment_controller' in globals():\n",
    "        del experiment_controller\n",
    "    \n",
    "    # Create new controller and display dashboard\n",
    "    experiment_controller = ExperimentController()\n",
    "    dashboard = experiment_controller.create_widgets()\n",
    "    \n",
    "    if dashboard:\n",
    "        display(dashboard)\n",
    "        print(\"âœ… Interactive Dashboard loaded successfully!\")\n",
    "        print(\"ðŸ“ Select an MLIR file and click 'Run Pipeline' to start.\")\n",
    "        print(\"ðŸ“„ File dropdown is now sorted alphabetically.\")\n",
    "    else:\n",
    "        print(\"âŒ Failed to create dashboard - no MLIR files found or other error.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Dashboard creation failed: {str(e)}\")\n",
    "    import traceback\n",
    "    print(\"Full error:\")\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bea2f4",
   "metadata": {},
   "source": [
    "## 3. NextPNR Execution and Analysis Functions\n",
    "\n",
    "Define functions to run FABulous preprocessing and nextpnr-himbaechel with comprehensive error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "384eec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 13 MLIR files through complete pipeline...\n",
      "Stage directories:\n",
      "  01_optimized_mlir: /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/01_optimized_mlir\n",
      "  02_extracted_mlir: /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/02_extracted_mlir\n",
      "  03_futil: /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/03_futil\n",
      "  04_verilog: /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/04_verilog\n",
      "  05_synthesis: /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/05_synthesis\n",
      "  06_PNR: /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/06_PNR\n",
      "\n",
      "[1/13] Processing: nw_nw.mlir\n",
      "Read 10429 characters from /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/01_optimized_mlir/nw_nw.mlir\n",
      "Successfully parsed with xDSL! Module has 1 operations\n",
      "Replacing i64/f64 types with i32/f32 types...\n",
      "Total types replaced: 0\n",
      "Found 1 scf.while loops to normalize\n",
      "  Normalizing scf.while loop 1/1\n",
      "Found function: nw_nw\n",
      "  Found 6 innermost loops\n",
      "  Replaced loop body of affine.for with call to nw_nw_inner_loop_0\n",
      "  Replaced loop body of affine.for with call to nw_nw_inner_loop_1\n",
      "  Replaced loop body of affine.for with call to nw_nw_inner_loop_2\n",
      "  Replaced loop body of scf.while with call to nw_nw_inner_loop_3\n",
      "  Replaced loop body of affine.for with call to nw_nw_inner_loop_4\n",
      "  Replaced loop body of affine.for with call to nw_nw_inner_loop_5\n",
      "  Applying type replacement to extracted function nw_nw_inner_loop_0\n",
      "Total types replaced: 0\n",
      "  Extracted function written to /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/02_extracted_mlir/nw_nw_inner_loop_0.mlir\n",
      "  Applying type replacement to extracted function nw_nw_inner_loop_1\n",
      "Total types replaced: 0\n",
      "  Extracted function written to /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/02_extracted_mlir/nw_nw_inner_loop_1.mlir\n",
      "  Applying type replacement to extracted function nw_nw_inner_loop_2\n",
      "Total types replaced: 0\n",
      "  Extracted function written to /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/02_extracted_mlir/nw_nw_inner_loop_2.mlir\n",
      "  Applying type replacement to extracted function nw_nw_inner_loop_3\n",
      "Total types replaced: 0\n",
      "  Extracted function written to /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/02_extracted_mlir/nw_nw_inner_loop_3.mlir\n",
      "  Applying type replacement to extracted function nw_nw_inner_loop_4\n",
      "Total types replaced: 0\n",
      "  Extracted function written to /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/02_extracted_mlir/nw_nw_inner_loop_4.mlir\n",
      "  Applying type replacement to extracted function nw_nw_inner_loop_5\n",
      "Total types replaced: 0\n",
      "  Extracted function written to /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/02_extracted_mlir/nw_nw_inner_loop_5.mlir\n",
      "Total extracted functions: 6\n",
      "  Result: âœ— FAILED (verilog_generation) | Runtime: 0.64s\n",
      "\n",
      "[2/13] Processing: gemm_ncubed.mlir\n",
      "Read 1659 characters from /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/01_optimized_mlir/gemm_ncubed.mlir\n",
      "Successfully parsed with xDSL! Module has 1 operations\n",
      "Replacing i64/f64 types with i32/f32 types...\n",
      "Total types replaced: 7\n",
      "Found 0 scf.while loops to normalize\n",
      "Found function: gemm_ncubed\n",
      "  Found 1 innermost loops\n",
      "  Replaced loop body of affine.for with call to gemm_ncubed_inner_loop_0\n",
      "  Applying type replacement to extracted function gemm_ncubed_inner_loop_0\n",
      "Total types replaced: 1\n",
      "  Extracted function written to /home/kelvin/FABulous_fork/myProject/PnR/compilation_result/02_extracted_mlir/gemm_ncubed_inner_loop_0.mlir\n",
      "Total extracted functions: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m mlir_files = \u001b[38;5;28mlist\u001b[39m(Path(BENCHMARK_ROOT_DIR).glob(\u001b[33m\"\u001b[39m\u001b[33m*.mlir\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Run complete pipeline for all MLIR files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m complete_results = \u001b[43mprocess_mlir_benchmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlir_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Complete Pipeline Results ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal files processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(complete_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 470\u001b[39m, in \u001b[36mprocess_mlir_benchmarks\u001b[39m\u001b[34m(mlir_files)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(mlir_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Processing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(mlir_file).name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m     result = \u001b[43mrun_complete_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlir_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    471\u001b[39m     results.append(result)\n\u001b[32m    473\u001b[39m     status = \u001b[33m\"\u001b[39m\u001b[33mâœ“ SUCCESS\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.overall_success \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ— FAILED (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.primary_failure_type.value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 412\u001b[39m, in \u001b[36mrun_complete_pipeline\u001b[39m\u001b[34m(mlir_file)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verilog_result.success \u001b[38;5;129;01mand\u001b[39;00m verilog_result.verilog_files:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m verilog_file \u001b[38;5;129;01min\u001b[39;00m verilog_result.verilog_files:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m         synthesis_results.append(\u001b[43mrun_synthesis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverilog_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    414\u001b[39m \u001b[38;5;66;03m# Stage 3: Parameter Sweep (if any synthesis succeeded)\u001b[39;00m\n\u001b[32m    415\u001b[39m parameter_results = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 251\u001b[39m, in \u001b[36mrun_synthesis\u001b[39m\u001b[34m(verilog_file)\u001b[39m\n\u001b[32m    249\u001b[39m p = os.environ.copy()\n\u001b[32m    250\u001b[39m p[\u001b[33m\"\u001b[39m\u001b[33mOUT_JSON_PATH\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(synthesis_out)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m synthesis_result = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynthesis_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m synthesis_runtime = time.time() - start_time\n\u001b[32m    254\u001b[39m synthesis_success = synthesis_result.returncode == \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:550\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    552\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:1209\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1206\u001b[39m     endtime = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1208\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1209\u001b[39m     stdout, stderr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1211\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1212\u001b[39m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[32m   1213\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:2115\u001b[39m, in \u001b[36mPopen._communicate\u001b[39m\u001b[34m(self, input, endtime, orig_timeout)\u001b[39m\n\u001b[32m   2108\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_timeout(endtime, orig_timeout,\n\u001b[32m   2109\u001b[39m                         stdout, stderr,\n\u001b[32m   2110\u001b[39m                         skip_check_and_raise=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[32m   2112\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   2113\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mfailed to raise TimeoutExpired.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2115\u001b[39m ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[38;5;28mself\u001b[39m._check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[32m   2118\u001b[39m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[32m   2119\u001b[39m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Main execution with unified pipeline\n",
    "mlir_files = list(Path(BENCHMARK_ROOT_DIR).glob(\"*.mlir\"))\n",
    "\n",
    "# Run complete pipeline for all MLIR files\n",
    "complete_results = process_mlir_benchmarks(mlir_files)\n",
    "\n",
    "print(f\"\\n=== Complete Pipeline Results ===\")\n",
    "print(f\"Total files processed: {len(complete_results)}\")\n",
    "\n",
    "# Display summary statistics\n",
    "successful_complete = sum(1 for r in complete_results if r.overall_success)\n",
    "print(f\"Successfully completed full pipeline: {successful_complete}/{len(complete_results)}\")\n",
    "\n",
    "# Stage-wise success rates\n",
    "verilog_success = sum(1 for r in complete_results if r.verilog_pipeline.success)\n",
    "synthesis_success = sum(1 for r in complete_results if any(s.success for s in r.synthesis_results))\n",
    "pnr_success = sum(1 for r in complete_results if any(p.success for p in r.parameter_results))\n",
    "\n",
    "print(f\"Verilog generation success: {verilog_success}/{len(complete_results)}\")\n",
    "print(f\"Synthesis success: {synthesis_success}/{len(complete_results)}\")\n",
    "print(f\"Parameter sweep success: {pnr_success}/{len(complete_results)}\")\n",
    "\n",
    "# Display detailed results for each file\n",
    "for result in complete_results:\n",
    "    print(f\"\\n--- {os.path.basename(result.source_file)} ---\")\n",
    "    print(f\"Overall success: {result.overall_success}\")\n",
    "    print(f\"Primary failure: {result.primary_failure_type.value}\")\n",
    "    print(f\"Total runtime: {result.total_runtime:.2f}s\")\n",
    "    \n",
    "    # Stage details\n",
    "    vp = result.verilog_pipeline\n",
    "    print(f\"  Verilog Pipeline: {'âœ“' if vp.success else 'âœ—'} ({vp.runtime:.2f}s)\")\n",
    "    print(f\"    MLIR Opt: {vp.mlir_optimization_time:.2f}s\")\n",
    "    print(f\"    Loop Extract: {vp.loop_extraction_time:.2f}s\") \n",
    "    print(f\"    FUTIL Gen: {vp.futil_generation_time:.2f}s\")\n",
    "    print(f\"    function extracted: {vp.function_extracted}\")\n",
    "    \n",
    "    # Synthesis results (now multiple)\n",
    "    if result.synthesis_results:\n",
    "        successful_synthesis = sum(1 for s in result.synthesis_results if s.success)\n",
    "        total_synthesis_time = sum(s.runtime for s in result.synthesis_results)\n",
    "        print(f\"  Synthesis: {successful_synthesis}/{len(result.synthesis_results)} successful ({total_synthesis_time:.2f}s total)\")\n",
    "        \n",
    "        # Show details for failed synthesis\n",
    "        for i, synth in enumerate(result.synthesis_results):\n",
    "            if not synth.success:\n",
    "                print(f\"    Failed synthesis {i+1}: {synth.error_message}\")\n",
    "    else:\n",
    "        print(\"  Synthesis: No synthesis attempted\")\n",
    "    \n",
    "    if result.parameter_results:\n",
    "        successful_params = sum(1 for p in result.parameter_results if p.success)\n",
    "        print(f\"  Parameter sweep: {successful_params}/{len(result.parameter_results)} configs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FABulous_fork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
